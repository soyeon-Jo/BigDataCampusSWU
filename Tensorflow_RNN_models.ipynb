{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_RNN models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV40U7mLZkN4"
      },
      "source": [
        "# Tensorflow 실습 : Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5X9HqjFaFVK"
      },
      "source": [
        "## RNN layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAgXBP8LtArB"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6madZY2m1zsO"
      },
      "source": [
        "- Tensorflow에서 제공하는 일반적인 RNN layers\n",
        "  - 기본 RNN: tf.keras.layers.SimpleRNN\n",
        "  - LSTM: tf.keras.layers.LSTM\n",
        "  - GRU: tf.keras.layers.GRU\n",
        "  \n",
        "- 기본적으로 RNN layer의 입력 tensor shape는 [batch, timesteps, feature]로 3D tensor\n",
        "  - batch: 한 번에 학습하는 데이터의 개수\n",
        "  - timesteps:입력 시퀀스의 길이(input_length)라고 표현. 즉, 시점의 수\n",
        "  - feature: 입력의 크기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR32V84D2MQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7035aee-9ffd-467c-f3f4-d70e4ceaf090"
      },
      "source": [
        "# 기본적으로 RNN layer의 입력 tensor shape는 [batch, timesteps, feature]로 3D tensor\n",
        "# 여기서 input은 16개의 time step을 가지고, 각 time step 마다 32차원의 값을 가지는 형태\n",
        "sequence_len = 16\n",
        "feature_dim = 32\n",
        "input_sequence = tf.keras.Input(shape=(sequence_len, feature_dim))\n",
        "\n",
        "# units은 RNN layer의 hidden size를 의미함\n",
        "units = 64\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# input sequence를 입력해서 모델을 build\n",
        "rnn(input_sequence)\n",
        "# RNN layer의 출력 shape를 보면, 시간 축이 사라진 것을 확인\n",
        "# 마지막 time step의 output을 return\n",
        "rnn.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 64)                6208      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g3De1ls6LoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ff2188-72fd-4b35-9c34-2c69f14ef083"
      },
      "source": [
        "# RNN layer를 정의할 때, return_sequences=True로 설정해주면 시간축이 유지되도록 return\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "# 시간 축에 따라, 모든 time step에 대한 output을 출력\n",
        "# 마지막 dense layer도 모든 time step에 대해 동일하게 적용됨\n",
        "rnn.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16, 4)             260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTRg8zBQF4Yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8848eb1d-725c-446f-ef7a-c17f483d7475"
      },
      "source": [
        "# RNN layer를 여러 층 쌓을 때 return_sequences=True가 꼭 필요함\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.SimpleRNN(units),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "rnn.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_3 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)     (None, 16, 64)            8256      \n",
            "_________________________________________________________________\n",
            "simple_rnn_5 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 22,980\n",
            "Trainable params: 22,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYKOhpnx7MCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb900d6-f04b-45b0-fcdb-8540be92f16a"
      },
      "source": [
        "# LSTM을 사용하는 방법도 동일함 \n",
        "lstm = tf.keras.Sequential([  \n",
        "  tf.keras.layers.LSTM(units),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "lstm(input_sequence)\n",
        "# 기본 RNN layer보다 훨씬 많은 파라미터 수를 가지고 있음\n",
        "lstm.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 25,092\n",
            "Trainable params: 25,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC80tzdo7kEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8dac4d-2a09-430b-f2f5-6bd2777101ea"
      },
      "source": [
        "# return_state=True 로 설정하면, LSTM의 마지막 cell state와 hidden state를 모두 return 받을 수 있음\n",
        "# tf.keras.Sequenctial에서는 각 layer의 output이 1개일때만 사용할수 있으므로, 적합하지 않음\n",
        "class LSTM(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):        \n",
        "        whole_seq_output, final_hidden_state, final_cell_state = self.lstm_layer(inputs)\n",
        "        \n",
        "        # shape를 출력\n",
        "        print(\"whole_seq_output :\", whole_seq_output.shape)\n",
        "        print(\"final_hidden_state :\", final_hidden_state.shape)\n",
        "        print(\"final_cell_state :\", final_cell_state.shape)\n",
        "\n",
        "        output_prob = self.dense(whole_seq_output)\n",
        "        return output_prob\n",
        "\n",
        "lstm = LSTM()\n",
        "lstm(input_sequence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_seq_output : (None, 16, 64)\n",
            "final_hidden_state : (None, 64)\n",
            "final_cell_state : (None, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 16, 4) dtype=float32 (created by layer 'lstm_1')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKYC7UzpBDaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7efba6-0f3e-4dd1-9086-903e4610329a"
      },
      "source": [
        "# GRU는 state가 하나로 되어있음\n",
        "class GRU(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(GRU, self).__init__()\n",
        "        self.gru_layer = tf.keras.layers.GRU(units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):        \n",
        "        whole_seq_output, final_state = self.gru_layer(inputs)\n",
        "\n",
        "        # shape를 출력\n",
        "        print(\"whole_seq_output :\", whole_seq_output.shape)\n",
        "        print(\"final_state :\", final_state.shape)\n",
        "        \n",
        "        output_prob = self.dense(whole_seq_output)\n",
        "        return output_prob\n",
        "\n",
        "gru = GRU()\n",
        "gru(input_sequence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_seq_output : (None, 16, 64)\n",
            "final_state : (None, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 16, 4) dtype=float32 (created by layer 'gru')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx6UkuneSmBU"
      },
      "source": [
        "## Bidirectional RNNs\n",
        ":두 개의 독립적인 RNN을 서로 합친 모델<br>\n",
        "참고 사이트<br>\n",
        "http://intelligence.korea.ac.kr/members/wschoi/bidirectional-rnn-in-pytorch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVL7Zu0nCXRk"
      },
      "source": [
        "- Bidirectional RNN layer를 만들기 위해서, Wrapper 형태로 활용가능한 layer를 제공함\n",
        "  - tf.keras.layers.Bidirectional\n",
        "- Bidirectional로 활용하고자 하는 RNN layer를 감싸주는 형태로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syhqwD6ZCW0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fff193d-7caa-4a8a-a08f-be5aed635e85"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)))\n",
        "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "model(input_sequence)\n",
        "# 양방향의 hidden state를 모두 출력하므로, output 차원이 2배 (64 * 2)\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 128)               49664     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 50,180\n",
            "Trainable params: 50,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIctpo6LIKiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d22b08e-49df-4676-cc1c-a227a662e435"
      },
      "source": [
        "# bidirectional RNN layer를 여러 층 쌓는 것도 가능\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)))\n",
        "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "model(input_sequence)\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection (None, 16, 128)           49664     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 148,996\n",
            "Trainable params: 148,996\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGNOzySqI-os"
      },
      "source": [
        "## TimeDistributed 이해 및 활용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu_95Z8fJO8i"
      },
      "source": [
        ":각 time에서 출력된 아웃풋을 내부에 선언해준 레이어와 연결시켜주는 역할\n",
        "- 시간 축에 대해 동일한 파라미터의 layer를 적용하는 방법\n",
        "- RNN layer와 직접적으로 관련된 것은 아니지만, sequence 데이터를 다루다보면 꼭 필요한 기능\n",
        "- 동일하게 적용할 layer를 wrapper로 감싸서 사용\n",
        "  - tf.keras.layers.TimeDistributed\n",
        "- Dense layer는 마지막 축(axis)에 대해 계산하므로, TimeDistributed가 자동으로 적용됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2lBvyfKJ6gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01340032-2581-4572-bc7b-9250e81d5bf1"
      },
      "source": [
        "# RNN layer의 output을 dense layer에 통과 시키는 것은 시간 축에 따라 동일한 파라미터를 활용하는 것임\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "rnn.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_6 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 16, 4)             260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBlveRc4KGQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bef7d5-d3fa-4151-e573-911b9b45864c"
      },
      "source": [
        "# TimeDistributed를 활용하는 예시\n",
        "# 위의 셀에서도 자동적으로 시간 축에 대해 동일한 파라미터를 사용하므로, 결과는 완전히 동일함\n",
        "rnn = tf.keras.Sequential([  \n",
        "  tf.keras.layers.SimpleRNN(units, return_sequences=True),\n",
        "  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "])\n",
        "\n",
        "rnn(input_sequence)\n",
        "rnn.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_7 (SimpleRNN)     (None, 16, 64)            6208      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 16, 4)             260       \n",
            "=================================================================\n",
            "Total params: 6,468\n",
            "Trainable params: 6,468\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gVCRhvwKuCx"
      },
      "source": [
        "- 아래는 TimeDistributed가 꼭 필요한 상황의 예시\n",
        "- 비디오 데이터 예시\n",
        "  - sequence length는 10\n",
        "  - 각 frame의 이미지는 128 x 128\n",
        "  - 각 이미지의 채널 수는 3\n",
        "- time step에 따라, 서로 다른 이미지에 동일한 CNN layer를 적용해야 하는 상황"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG79loPTKsbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64b5f48-71cf-47ac-84e7-b44f02f66f5a"
      },
      "source": [
        "# input shape : [sequence length, width, height, channel]\n",
        "inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3)))\n",
        "])\n",
        "\n",
        "model(inputs)\n",
        "# 모든 time step에 대해 동일한 cnn layer 적용된 것 확인\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_1 (TimeDist (None, 10, 126, 126, 64)  1792      \n",
            "=================================================================\n",
            "Total params: 1,792\n",
            "Trainable params: 1,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}